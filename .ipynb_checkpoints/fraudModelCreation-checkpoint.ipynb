{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikeb\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mPython Version: 3.6.2\u001b[0m\n",
      "\u001b[35mTensorFlow Ver: 1.6.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "from termcolor import colored\n",
    "print(colored('Python Version: %s' % sys.version.split()[0], 'blue'))\n",
    "print(colored('TensorFlow Ver: %s' % tf.__version__, 'magenta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter no. of epochs for RNN training: 500\n"
     ]
    }
   ],
   "source": [
    "n_epoch = int(input('Enter no. of epochs for RNN training: '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mNo. of epochs: 500\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(colored('No. of epochs: %d' % n_epoch, 'red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fake/real</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>We stayed 2 nights over spring break in what w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Stayed at the Fitzpatrick as a result of all o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>We booked this hotel as a last minute vacation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>This hotel is a shambles-furniture literally f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>The hotel itself was beautiful and wonderful s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>I'd been searching for a cool, non-chain hotel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>I stayed @ Affinia for one night on a purpose ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Having stayed at the excellent Affinia chain i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>I've been here for 4 days. Great location righ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Stayed at the Fitzpatrick before, about 4 year...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  fake/real                                            reviews\n",
       "0           0          1  We stayed 2 nights over spring break in what w...\n",
       "1           1          1  Stayed at the Fitzpatrick as a result of all o...\n",
       "2           2          1  We booked this hotel as a last minute vacation...\n",
       "3           3          1  This hotel is a shambles-furniture literally f...\n",
       "4           4          1  The hotel itself was beautiful and wonderful s...\n",
       "5           5          1  I'd been searching for a cool, non-chain hotel...\n",
       "6           6          1  I stayed @ Affinia for one night on a purpose ...\n",
       "7           7          1  Having stayed at the excellent Affinia chain i...\n",
       "8           8          1  I've been here for 4 days. Great location righ...\n",
       "9           9          1  Stayed at the Fitzpatrick before, about 4 year..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('fraud_traing_set.csv')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape: (1600, 3)\n"
     ]
    }
   ],
   "source": [
    "print('data.shape:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    800\n",
       "0    800\n",
       "Name: fake/real, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"fake/real\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.reviews\n",
    "y = data[\"fake/real\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    We stayed 2 nights over spring break in what w...\n",
       "1    Stayed at the Fitzpatrick as a result of all o...\n",
       "2    We booked this hotel as a last minute vacation...\n",
       "3    This hotel is a shambles-furniture literally f...\n",
       "4    The hotel itself was beautiful and wonderful s...\n",
       "5    I'd been searching for a cool, non-chain hotel...\n",
       "6    I stayed @ Affinia for one night on a purpose ...\n",
       "7    Having stayed at the excellent Affinia chain i...\n",
       "8    I've been here for 4 days. Great location righ...\n",
       "9    Stayed at the Fitzpatrick before, about 4 year...\n",
       "Name: reviews, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "5    1\n",
       "6    1\n",
       "7    1\n",
       "8    1\n",
       "9    1\n",
       "Name: fake/real, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(stop_words='english', token_pattern=r'\\b\\w{2,}\\b')\n",
    "dummy = DummyClassifier(strategy='most_frequent', random_state=0)\n",
    "\n",
    "dummy_pipeline = make_pipeline(vect, dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dummyclassifier': DummyClassifier(constant=None, random_state=0, strategy='most_frequent'),\n",
       " 'tfidfvectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='\\\\b\\\\w{2,}\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_pipeline.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\n",
      "Dummy Classifier's Accuracy: 0.50000\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation\n",
    "cv = cross_val_score(dummy_pipeline, X, y, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "print(colored('\\nDummy Classifier\\'s Accuracy: %0.5f\\n' % cv.mean(), 'yellow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(stop_words='english', \n",
    "                       token_pattern=r'\\b\\w{2,}\\b',\n",
    "                       min_df=1, max_df=0.1,\n",
    "                       ngram_range=(1,2))\n",
    "mnb = MultinomialNB(alpha=2)\n",
    "\n",
    "mnb_pipeline = make_pipeline(vect, mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multinomialnb': MultinomialNB(alpha=2, class_prior=None, fit_prior=True),\n",
       " 'tfidfvectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=0.1, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='\\\\b\\\\w{2,}\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_pipeline.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "MultinomialNB Classifier's Accuracy: 0.86000\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation\n",
    "cv = cross_val_score(mnb_pipeline, X, y, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "print(colored('\\nMultinomialNB Classifier\\'s Accuracy: %0.5f\\n' % cv.mean(), 'green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(ngram_range=(1,1), token_pattern=r'\\b\\w{1,}\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect.fit(X_train)\n",
    "vocab = vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_X_to_X_word_ids(X):\n",
    "    return X.apply( lambda x: [vocab[w] for w in [w.lower().strip() for w in x.split()] if w in vocab] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_word_ids = convert_X_to_X_word_ids(X_train)\n",
    "X_test_word_ids  = convert_X_to_X_word_ids(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1379    As a frequent traveler for both business and p...\n",
       "938     A few weeks ago we stayed at the Hard Rock Hot...\n",
       "1197    I'd expect a \"luxury\" hotel to pay more attent...\n",
       "730     I stayed for just one nite at the Sheraton Hot...\n",
       "1145    After arriving at the Sofitel Chicago Water To...\n",
       "Name: reviews, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1379    [753, 287, 3485, 8367, 3415, 1232, 1374, 591, ...\n",
       "938     [287, 3255, 8932, 484, 8907, 7709, 804, 8126, ...\n",
       "1197    [3099, 287, 4087, 8252, 5871, 5287, 831, 8252,...\n",
       "730     [4152, 7709, 3415, 4536, 5625, 5461, 804, 8126...\n",
       "1145    [464, 741, 804, 8126, 7496, 1626, 8890, 8317, ...\n",
       "Name: reviews, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_word_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_word_ids.shape: (1440,)\n",
      "X_test_word_ids.shape: (160,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train_word_ids.shape:', X_train_word_ids.shape)\n",
    "print('X_test_word_ids.shape:', X_test_word_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_padded_seqs = pad_sequences(X_train_word_ids, maxlen=20, value=0)\n",
    "X_test_padded_seqs  = pad_sequences(X_test_word_ids , maxlen=20, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_padded_seqs.shape: (1440, 20)\n",
      "X_test_padded_seqs.shape: (160, 20)\n"
     ]
    }
   ],
   "source": [
    "print('X_train_padded_seqs.shape:', X_train_padded_seqs.shape)\n",
    "print('X_test_padded_seqs.shape:', X_test_padded_seqs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>753</td>\n",
       "      <td>287</td>\n",
       "      <td>3485</td>\n",
       "      <td>8367</td>\n",
       "      <td>3415</td>\n",
       "      <td>1232</td>\n",
       "      <td>1374</td>\n",
       "      <td>591</td>\n",
       "      <td>1703</td>\n",
       "      <td>591</td>\n",
       "      <td>1810</td>\n",
       "      <td>331</td>\n",
       "      <td>804</td>\n",
       "      <td>287</td>\n",
       "      <td>6449</td>\n",
       "      <td>6200</td>\n",
       "      <td>706</td>\n",
       "      <td>4211</td>\n",
       "      <td>8252</td>\n",
       "      <td>4036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>287</td>\n",
       "      <td>3255</td>\n",
       "      <td>8932</td>\n",
       "      <td>484</td>\n",
       "      <td>8907</td>\n",
       "      <td>7709</td>\n",
       "      <td>804</td>\n",
       "      <td>8126</td>\n",
       "      <td>3878</td>\n",
       "      <td>6834</td>\n",
       "      <td>4087</td>\n",
       "      <td>4228</td>\n",
       "      <td>8126</td>\n",
       "      <td>4087</td>\n",
       "      <td>8870</td>\n",
       "      <td>287</td>\n",
       "      <td>1376</td>\n",
       "      <td>3509</td>\n",
       "      <td>8126</td>\n",
       "      <td>3615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3099</td>\n",
       "      <td>287</td>\n",
       "      <td>4087</td>\n",
       "      <td>8252</td>\n",
       "      <td>5871</td>\n",
       "      <td>5287</td>\n",
       "      <td>831</td>\n",
       "      <td>8252</td>\n",
       "      <td>8126</td>\n",
       "      <td>6859</td>\n",
       "      <td>6444</td>\n",
       "      <td>1039</td>\n",
       "      <td>8178</td>\n",
       "      <td>4152</td>\n",
       "      <td>3455</td>\n",
       "      <td>3831</td>\n",
       "      <td>8520</td>\n",
       "      <td>8126</td>\n",
       "      <td>8126</td>\n",
       "      <td>5218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4152</td>\n",
       "      <td>7709</td>\n",
       "      <td>3415</td>\n",
       "      <td>4536</td>\n",
       "      <td>5625</td>\n",
       "      <td>5461</td>\n",
       "      <td>804</td>\n",
       "      <td>8126</td>\n",
       "      <td>7209</td>\n",
       "      <td>4087</td>\n",
       "      <td>591</td>\n",
       "      <td>8319</td>\n",
       "      <td>591</td>\n",
       "      <td>8870</td>\n",
       "      <td>287</td>\n",
       "      <td>4811</td>\n",
       "      <td>5414</td>\n",
       "      <td>464</td>\n",
       "      <td>6434</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>464</td>\n",
       "      <td>741</td>\n",
       "      <td>804</td>\n",
       "      <td>8126</td>\n",
       "      <td>7496</td>\n",
       "      <td>1626</td>\n",
       "      <td>8890</td>\n",
       "      <td>8317</td>\n",
       "      <td>4087</td>\n",
       "      <td>4152</td>\n",
       "      <td>8870</td>\n",
       "      <td>3754</td>\n",
       "      <td>9027</td>\n",
       "      <td>6893</td>\n",
       "      <td>591</td>\n",
       "      <td>5347</td>\n",
       "      <td>6859</td>\n",
       "      <td>6688</td>\n",
       "      <td>3823</td>\n",
       "      <td>1039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4     5     6     7     8     9     10    11  \\\n",
       "0   753   287  3485  8367  3415  1232  1374   591  1703   591  1810   331   \n",
       "1   287  3255  8932   484  8907  7709   804  8126  3878  6834  4087  4228   \n",
       "2  3099   287  4087  8252  5871  5287   831  8252  8126  6859  6444  1039   \n",
       "3  4152  7709  3415  4536  5625  5461   804  8126  7209  4087   591  8319   \n",
       "4   464   741   804  8126  7496  1626  8890  8317  4087  4152  8870  3754   \n",
       "\n",
       "     12    13    14    15    16    17    18    19  \n",
       "0   804   287  6449  6200   706  4211  8252  4036  \n",
       "1  8126  4087  8870   287  1376  3509  8126  3615  \n",
       "2  8178  4152  3455  3831  8520  8126  8126  5218  \n",
       "3   591  8870   287  4811  5414   464  6434   287  \n",
       "4  9027  6893   591  5347  6859  6688  3823  1039  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train_padded_seqs).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7477</td>\n",
       "      <td>5347</td>\n",
       "      <td>5073</td>\n",
       "      <td>5696</td>\n",
       "      <td>8520</td>\n",
       "      <td>8126</td>\n",
       "      <td>5582</td>\n",
       "      <td>2510</td>\n",
       "      <td>804</td>\n",
       "      <td>8126</td>\n",
       "      <td>1642</td>\n",
       "      <td>8650</td>\n",
       "      <td>737</td>\n",
       "      <td>3912</td>\n",
       "      <td>8079</td>\n",
       "      <td>5073</td>\n",
       "      <td>4009</td>\n",
       "      <td>1256</td>\n",
       "      <td>706</td>\n",
       "      <td>8889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8907</td>\n",
       "      <td>7709</td>\n",
       "      <td>804</td>\n",
       "      <td>8126</td>\n",
       "      <td>6810</td>\n",
       "      <td>1472</td>\n",
       "      <td>8466</td>\n",
       "      <td>8932</td>\n",
       "      <td>591</td>\n",
       "      <td>4228</td>\n",
       "      <td>1848</td>\n",
       "      <td>8126</td>\n",
       "      <td>7496</td>\n",
       "      <td>2636</td>\n",
       "      <td>5501</td>\n",
       "      <td>1805</td>\n",
       "      <td>9027</td>\n",
       "      <td>8126</td>\n",
       "      <td>2295</td>\n",
       "      <td>8126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4152</td>\n",
       "      <td>1412</td>\n",
       "      <td>8126</td>\n",
       "      <td>6688</td>\n",
       "      <td>4782</td>\n",
       "      <td>8252</td>\n",
       "      <td>1211</td>\n",
       "      <td>287</td>\n",
       "      <td>6859</td>\n",
       "      <td>804</td>\n",
       "      <td>8126</td>\n",
       "      <td>1052</td>\n",
       "      <td>5582</td>\n",
       "      <td>2315</td>\n",
       "      <td>111</td>\n",
       "      <td>3415</td>\n",
       "      <td>287</td>\n",
       "      <td>6412</td>\n",
       "      <td>9027</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8174</td>\n",
       "      <td>4451</td>\n",
       "      <td>287</td>\n",
       "      <td>8300</td>\n",
       "      <td>5625</td>\n",
       "      <td>8252</td>\n",
       "      <td>753</td>\n",
       "      <td>8126</td>\n",
       "      <td>7678</td>\n",
       "      <td>6417</td>\n",
       "      <td>8947</td>\n",
       "      <td>3509</td>\n",
       "      <td>14</td>\n",
       "      <td>7678</td>\n",
       "      <td>8252</td>\n",
       "      <td>218</td>\n",
       "      <td>591</td>\n",
       "      <td>3823</td>\n",
       "      <td>8252</td>\n",
       "      <td>3269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4152</td>\n",
       "      <td>8870</td>\n",
       "      <td>6444</td>\n",
       "      <td>3068</td>\n",
       "      <td>8965</td>\n",
       "      <td>8907</td>\n",
       "      <td>1212</td>\n",
       "      <td>287</td>\n",
       "      <td>5443</td>\n",
       "      <td>804</td>\n",
       "      <td>8126</td>\n",
       "      <td>3878</td>\n",
       "      <td>6834</td>\n",
       "      <td>4087</td>\n",
       "      <td>4228</td>\n",
       "      <td>4458</td>\n",
       "      <td>8870</td>\n",
       "      <td>382</td>\n",
       "      <td>1106</td>\n",
       "      <td>5693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4     5     6     7     8     9     10    11  \\\n",
       "0  7477  5347  5073  5696  8520  8126  5582  2510   804  8126  1642  8650   \n",
       "1  8907  7709   804  8126  6810  1472  8466  8932   591  4228  1848  8126   \n",
       "2  4152  1412  8126  6688  4782  8252  1211   287  6859   804  8126  1052   \n",
       "3  8174  4451   287  8300  5625  8252   753  8126  7678  6417  8947  3509   \n",
       "4  4152  8870  6444  3068  8965  8907  1212   287  5443   804  8126  3878   \n",
       "\n",
       "     12    13    14    15    16    17    18    19  \n",
       "0   737  3912  8079  5073  4009  1256   706  8889  \n",
       "1  7496  2636  5501  1805  9027  8126  2295  8126  \n",
       "2  5582  2315   111  3415   287  6412  9027   287  \n",
       "3    14  7678  8252   218   591  3823  8252  3269  \n",
       "4  6834  4087  4228  4458  8870   382  1106  5693  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_test_padded_seqs).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_y_labels = list(y_train.value_counts().index)\n",
    "unique_y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(unique_y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 0\n",
      "1: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('')\n",
    "for label_id, label_name in zip(le.transform(unique_y_labels), unique_y_labels):\n",
    "    print('%d: %s' % (label_id, label_name))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train.map(lambda x: le.transform([x])[0]), nb_classes=len(unique_y_labels))\n",
    "y_test  = to_categorical(y_test.map(lambda x:  le.transform([x])[0]), nb_classes=len(unique_y_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape: (1440, 2)\n",
      "y_test.shape: (160, 2)\n"
     ]
    }
   ],
   "source": [
    "print('y_train.shape:', y_train.shape)\n",
    "print('y_test.shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size_of_each_vector = X_train_padded_seqs.shape[1]\n",
    "vocab_size = len(vocab)\n",
    "no_of_unique_y_labels = len(unique_y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size_of_each_vector: 20\n",
      "vocab_size: 9150\n",
      "no_of_unique_y_labels: 2\n"
     ]
    }
   ],
   "source": [
    "print('size_of_each_vector:', size_of_each_vector)\n",
    "print('vocab_size:', vocab_size)\n",
    "print('no_of_unique_y_labels:', no_of_unique_y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mikeb\\Anaconda3\\lib\\site-packages\\tflearn\\objectives.py:66: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#sgd = tflearn.SGD(learning_rate=1e-4, lr_decay=0.96, decay_step=1000)\n",
    "\n",
    "net = tflearn.input_data([None, size_of_each_vector]) # The first element is the \"batch size\" which we set to \"None\"\n",
    "net = tflearn.embedding(net, input_dim=vocab_size, output_dim=128) # input_dim: vocabulary size\n",
    "net = tflearn.lstm(net, 128, dropout=0.6) # Set the dropout to 0.6\n",
    "net = tflearn.fully_connected(net, no_of_unique_y_labels, activation='softmax') # relu or softmax\n",
    "net = tflearn.regression(net, \n",
    "                         optimizer='adam',  # adam or ada or adagrad # sgd\n",
    "                         learning_rate=1e-4,\n",
    "                         loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = tflearn.DNN(net, tensorboard_verbose=0, checkpoint_path='SavedModels/model.tfl.ckpt')\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: D41JL6\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 1440\n",
      "Validation samples: 160\n",
      "--\n",
      "Training Step: 1  | time: 2.190s\n",
      "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 | val_loss: 0.69326 - val_acc: 0.3875 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.62382\u001b[0m\u001b[0m | time: 1.447s\n",
      "| Adam | epoch: 002 | loss: 0.62382 - acc: 0.4600 | val_loss: 0.69329 - val_acc: 0.3875 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m0.68050\u001b[0m\u001b[0m | time: 1.425s\n",
      "| Adam | epoch: 003 | loss: 0.68050 - acc: 0.5018 | val_loss: 0.69332 - val_acc: 0.4375 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m0.68999\u001b[0m\u001b[0m | time: 1.437s\n",
      "| Adam | epoch: 004 | loss: 0.68999 - acc: 0.5005 | val_loss: 0.69335 - val_acc: 0.4313 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m0.69212\u001b[0m\u001b[0m | time: 1.434s\n",
      "| Adam | epoch: 005 | loss: 0.69212 - acc: 0.5174 | val_loss: 0.69338 - val_acc: 0.4125 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m0.69275\u001b[0m\u001b[0m | time: 1.428s\n",
      "| Adam | epoch: 006 | loss: 0.69275 - acc: 0.5134 | val_loss: 0.69341 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m0.69291\u001b[0m\u001b[0m | time: 1.431s\n",
      "| Adam | epoch: 007 | loss: 0.69291 - acc: 0.5291 | val_loss: 0.69344 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m0.69302\u001b[0m\u001b[0m | time: 1.432s\n",
      "| Adam | epoch: 008 | loss: 0.69302 - acc: 0.5119 | val_loss: 0.69347 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m0.69298\u001b[0m\u001b[0m | time: 1.435s\n",
      "| Adam | epoch: 009 | loss: 0.69298 - acc: 0.5159 | val_loss: 0.69350 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m0.69304\u001b[0m\u001b[0m | time: 1.423s\n",
      "| Adam | epoch: 010 | loss: 0.69304 - acc: 0.5111 | val_loss: 0.69353 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m0.69297\u001b[0m\u001b[0m | time: 1.429s\n",
      "| Adam | epoch: 011 | loss: 0.69297 - acc: 0.5121 | val_loss: 0.69356 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m0.69305\u001b[0m\u001b[0m | time: 1.424s\n",
      "| Adam | epoch: 012 | loss: 0.69305 - acc: 0.5101 | val_loss: 0.69359 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m0.69299\u001b[0m\u001b[0m | time: 1.447s\n",
      "| Adam | epoch: 013 | loss: 0.69299 - acc: 0.5099 | val_loss: 0.69363 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m0.69304\u001b[0m\u001b[0m | time: 1.457s\n",
      "| Adam | epoch: 014 | loss: 0.69304 - acc: 0.5098 | val_loss: 0.69366 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m0.69296\u001b[0m\u001b[0m | time: 1.437s\n",
      "| Adam | epoch: 015 | loss: 0.69296 - acc: 0.5095 | val_loss: 0.69369 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m0.69302\u001b[0m\u001b[0m | time: 1.456s\n",
      "| Adam | epoch: 016 | loss: 0.69302 - acc: 0.5093 | val_loss: 0.69372 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m0.69294\u001b[0m\u001b[0m | time: 1.458s\n",
      "| Adam | epoch: 017 | loss: 0.69294 - acc: 0.5092 | val_loss: 0.69376 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m0.69298\u001b[0m\u001b[0m | time: 1.463s\n",
      "| Adam | epoch: 018 | loss: 0.69298 - acc: 0.5092 | val_loss: 0.69379 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m0.69292\u001b[0m\u001b[0m | time: 1.433s\n",
      "| Adam | epoch: 019 | loss: 0.69292 - acc: 0.5091 | val_loss: 0.69383 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m0.69297\u001b[0m\u001b[0m | time: 1.470s\n",
      "| Adam | epoch: 020 | loss: 0.69297 - acc: 0.5091 | val_loss: 0.69386 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m0.69288\u001b[0m\u001b[0m | time: 1.438s\n",
      "| Adam | epoch: 021 | loss: 0.69288 - acc: 0.5091 | val_loss: 0.69390 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m0.69292\u001b[0m\u001b[0m | time: 1.459s\n",
      "| Adam | epoch: 022 | loss: 0.69292 - acc: 0.5091 | val_loss: 0.69394 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m0.69284\u001b[0m\u001b[0m | time: 1.490s\n",
      "| Adam | epoch: 023 | loss: 0.69284 - acc: 0.5090 | val_loss: 0.69397 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m0.69290\u001b[0m\u001b[0m | time: 1.431s\n",
      "| Adam | epoch: 024 | loss: 0.69290 - acc: 0.5090 | val_loss: 0.69401 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m0.69282\u001b[0m\u001b[0m | time: 1.431s\n",
      "| Adam | epoch: 025 | loss: 0.69282 - acc: 0.5090 | val_loss: 0.69404 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m0.69288\u001b[0m\u001b[0m | time: 1.425s\n",
      "| Adam | epoch: 026 | loss: 0.69288 - acc: 0.5090 | val_loss: 0.69408 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m0.69279\u001b[0m\u001b[0m | time: 1.433s\n",
      "| Adam | epoch: 027 | loss: 0.69279 - acc: 0.5090 | val_loss: 0.69411 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m0.69286\u001b[0m\u001b[0m | time: 1.426s\n",
      "| Adam | epoch: 028 | loss: 0.69286 - acc: 0.5090 | val_loss: 0.69415 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m0.69276\u001b[0m\u001b[0m | time: 1.430s\n",
      "| Adam | epoch: 029 | loss: 0.69276 - acc: 0.5090 | val_loss: 0.69419 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m0.69283\u001b[0m\u001b[0m | time: 1.432s\n",
      "| Adam | epoch: 030 | loss: 0.69283 - acc: 0.5090 | val_loss: 0.69422 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m0.69274\u001b[0m\u001b[0m | time: 1.464s\n",
      "| Adam | epoch: 031 | loss: 0.69274 - acc: 0.5090 | val_loss: 0.69425 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m0.69281\u001b[0m\u001b[0m | time: 1.429s\n",
      "| Adam | epoch: 032 | loss: 0.69281 - acc: 0.5090 | val_loss: 0.69429 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m0.69269\u001b[0m\u001b[0m | time: 1.406s\n",
      "| Adam | epoch: 033 | loss: 0.69269 - acc: 0.5090 | val_loss: 0.69432 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m0.69277\u001b[0m\u001b[0m | time: 1.475s\n",
      "| Adam | epoch: 034 | loss: 0.69277 - acc: 0.5090 | val_loss: 0.69435 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m0.69265\u001b[0m\u001b[0m | time: 1.463s\n",
      "| Adam | epoch: 035 | loss: 0.69265 - acc: 0.5090 | val_loss: 0.69439 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m0.69273\u001b[0m\u001b[0m | time: 1.422s\n",
      "| Adam | epoch: 036 | loss: 0.69273 - acc: 0.5090 | val_loss: 0.69442 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m0.69260\u001b[0m\u001b[0m | time: 1.450s\n",
      "| Adam | epoch: 037 | loss: 0.69260 - acc: 0.5090 | val_loss: 0.69446 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m0.69248\u001b[0m\u001b[0m | time: 1.433s\n",
      "| Adam | epoch: 038 | loss: 0.69248 - acc: 0.5090 | val_loss: 0.69449 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m0.69239\u001b[0m\u001b[0m | time: 1.465s\n",
      "| Adam | epoch: 039 | loss: 0.69239 - acc: 0.5090 | val_loss: 0.69452 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m0.69251\u001b[0m\u001b[0m | time: 1.467s\n",
      "| Adam | epoch: 040 | loss: 0.69251 - acc: 0.5090 | val_loss: 0.69455 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m0.69237\u001b[0m\u001b[0m | time: 1.438s\n",
      "| Adam | epoch: 041 | loss: 0.69237 - acc: 0.5090 | val_loss: 0.69457 - val_acc: 0.4187 -- iter: 1440/1440\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_padded_seqs, y_train, \n",
    "          validation_set=(X_test_padded_seqs, y_test), \n",
    "          n_epoch=n_epoch,\n",
    "          show_metric=True, \n",
    "          batch_size=1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('SavedModels/model.tfl')\n",
    "print(colored('Model Saved!', 'red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load('SavedModels/model.tfl')\n",
    "print(colored('Model Loaded!', 'red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_classes = [np.argmax(i) for i in model.predict(X_test_padded_seqs)]\n",
    "true_classes = [np.argmax(i) for i in y_test]\n",
    "\n",
    "print(colored('\\nRNN Classifier\\'s Accuracy: %0.5f\\n' % metrics.accuracy_score(true_classes, pred_classes), 'cyan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_of_titles = range(0,200) # range(X_test.shape[0]) \n",
    "\n",
    "for i in ids_of_titles:\n",
    "    pred_class = np.argmax(model.predict([X_test_padded_seqs[i]]))\n",
    "    true_class = np.argmax(y_test[i])\n",
    "    \n",
    "    print(X_test.values[i])\n",
    "    print('pred_class:', le.inverse_transform(pred_class))\n",
    "    print('true_class:', le.inverse_transform(true_class))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
